---
title: "Projet climat LBP"
output: html_notebook
authors: "Laurent BIGAS, Mathieu DA SILVA, Killian BOULARD"
---

## Déclaration des librairies
```{r}
library(tidymodels)
library(tidyverse)
```

```{r setup}
knitr::opts_knit$set(root.dir = "c:/Users/User/Desktop/Cours et documents/formation R ensae/DATA")
```

## Import des données

```{r}
incendies <- read.csv(file="Incendies.csv", header = T, sep=";", skip = 6,  encoding="UTF-8")
inc_dataset = rename(incendies,
                   "annee" = Année,
                   "numero" = Numéro,
                   "departement" = Département,
                   "code_insee" = Code.INSEE,
                   "nom_commune" = Nom.de.la.commune,
                   "date_alerte" = Date.de.première.alerte,
                   "origine_alerte" = Origine.de.l.alerte,
                   "moyens_premiere_intervention" = Moyens.de.première.intervention,
                   "surface_parcourue" = Surface.parcourue..m2.,
                   "surface_foret" = Surface.forêt..m2.,
                   "surface_maquis" = Surface.maquis.garrigues..m2.,
                   "surface_nat_autre_foret" = Autres.surfaces.naturelles.hors.forêt..m2.,
                   "surface_agricole" = Surfaces.agricoles..m2.,
                   "surface_autre_terre_boisee" =  Surface.autres.terres.boisées..m2.,
                   "surface_non_boisee_nat" = Surfaces.non.boisées.naturelles..m2.,
                   "surface_non_boisee_art" = Surfaces.non.boisées.artificialisées..m2.,
                   "surface_non_boisee" = Surfaces.non.boisées..m2.,
                   "precision_surf" = Précision.des.surfaces,
                   "surface_feu_initiale" = Surface.de.feu.à.l.arrivée.des.secours...0.1.ha,
                   "voie_caross_proche" = Voie.carrossable.la.plus.proche,
                   "act_hab_proche" = Activité.ou.habitation.la.plus.proche,
                   "type_peupl" = Type.de.peuplement,
                   "connaissance" = Connaissance,
                   "source_enquete" = Source.de.l.enquête,
                   "nature" = Nature,
                   "enterv_equipe" = Intervention.de.l.équipe.RCCI,
                   "deces_bat_touches" = Décès.ou.bâtiments.touchés,
                   "nb_deces" = Nombre.de.décès,
                   "nb_bat_tot_detruit" = Nombre.de.bâtiments.totalement.détruits,
                   "nb_bat_part_detruit" = Nombre.de.bâtiments.partiellement.détruits,
                   "hygrometrie" = Hygrométrie,
                   "v_moyenn_vent" = Vitesse.moyenne.du.vent,
                   "dir_vent" = Direction.du.vent,
                   "precision_donnee" = Précision.de.la.donnée,
                   "presence_contour_valide" = Présence.d.un.contour.valide
)

glimpse(inc_dataset)
```

## Exploration des données
- Vérification des valeurs manquantes ;
- Tableaux / plots ; 
- Renommage des colonnes.

```{r}
is.na(inc_dataset)
colSums(is.na(inc_dataset))
apply(is.na(inc_dataset), MARGIN = 2, sum)
dim(inc_dataset)
```

## Data preprocessing Part I

```{r}
inc_dataset = inc_dataset %>%
  filter(!is.na(id_D)) %>%
  mutate(target = surface_parcourue) %>%
  mutate_at(c('origine_alerte', 'nature', 'dir_vent'), as.factor) %>% #as.ch
  select(-c(nb_bat_tot_detruit, nb_bat_part_detruit,
            nb_deces, surface_agricole)) %>%
  mutate(nbda = if_else(is.na(nbda), 0, as.numeric(nbda))) %>%
  filter(duree>1 & modeSortie !=9)

dataset
```

## Resample library : training set, validation

```{r}
set.seed(42)
dataset_split = initial_split(dataset, prop = 0.8, strata = target)
training = training(dataset_split)
test_set = testing(dataset_split)

training

set.seed(42)
train_split = initial_split(training, prop = 0.8, strata = target)
train_set = training(train_split)
eval_set = testing(train_split)
train_set

cat(dim(train_set),'/',dim(eval_set),'/',dim(test_set))
```

## 2 Recipes library : create a collection of recipes
### 2.1 Basic recipe
```{r}
rec_basic = recipe(data = train_set, target~.) %>%
  step_impute_mode(sexe) %>%
  step_impute_mean(age) %>%
  step_normalize(age) %>% #Centre et réduit, step_scale = juste réduire
  step_other(ghm2, threshold = 0.05) %>%
  step_dummy(ghm2) #Encodage 0 ou 1

rec_basic
prep(rec_basic)

juice(prep(rec_basic)) #Design matrix
formula(prep(rec_basic))
```

Version plus généralisable

```{r}
basic_rec = recipe(data= train_set, target~.) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_other(ghm2, threshold = 0.02) %>%
  step_other(dp, threshold = 0.05) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) #suppression des catégorie identiques (toujours egale 0) notamment a cause des facteurs

juice(prep(basic_rec))
```

### 2.2 Interaction recipe

```{r}
rec_interaction = basic_rec %>% 
                  step_interact(~age : starts_with('dp'))

juice(prep(rec_interaction))
```

### 2.3 Spline recipe

```{r}
rec_spline = 
  # basic_rec %>% 
  rec_interaction %>% 
  step_ns(age, duree, deg_free = 3) #On peut aussi mettre tune()
juice(prep(rec_spline))
```



## 3. Parsnip library : creating model and fitting
### 3.1 logistical model and workflow

model is 
* a model 
* a engine (package)
* a mode (classification / regression)

```{r}
log_mod = logistic_reg() %>%
  set_engine('glm')
  set_mode('classification')
```

wf is
* declarer un workflow
* ajouter un recipe
* ajouter un modèle

```{r}
log_wf = workflow() %>%
  add_recipe(rec_basic) %>%
  add_model(log_mod)

log_wf

log_fit = fit(log_wf, train_set)
              
predict(log_fit, eval_set)
predict(log_fit, eval_set, type = 'prob')


log_pred = eval_set %>%
  select(target) %>%
  bind_cols(
    predict(log_fit, eval_set),
    predict(log_fit, eval_set, type = 'prob')
  )

log_pred
```

### 3.2 RandomForest model and workflow
```{r}
rf_mod = rand_forest() %>% 
  set_engine('ranger') %>% 
  set_mode('classification')
```


```{r}
rf_wf = workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(rf_mod)

rf_fit = fit(rf_wf, train_set)

rf_fit %>% 
  predict(eval_set, type = 'prob')

rf_fit %>% 
  predict(eval_set)

rf_pred = eval_set %>% 
  select(target) %>% 
  bind_cols(
    rf_fit %>% 
  predict(eval_set, type = 'prob'),
    rf_fit %>% 
  predict(eval_set)
  )

rf_pred

```

### 3.3 Xgboost

```{r}
xg_mod = boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),
                     loss_reduction = tune()) %>% 
  set_engine('xgboost') %>% 
  set_mode('classification')
```

```{r}
rf_wf = workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(rf_mod)

rf_fit = fit(rf_wf, train_set)

rf_fit %>% 
  predict(eval_set, type = 'prob')

rf_fit %>% 
  predict(eval_set)

rf_pred = eval_set %>% 
  select(target) %>% 
  bind_cols(
    rf_fit %>% 
  predict(eval_set, type = 'prob'),
    rf_fit %>% 
  predict(eval_set)
  )

rf_pred
```

## 4. Yardstick library : Evaluer les performances des modèles
###4.1 Evaluer log

```{r}
log_pred

accuracy(log_pred, target, .pred_class)

log_pred %>% 
  group_by(target, .pred_class) %>% 
  summarise(n=n())
  
roc_auc(log_pred, target, .pred_0) #mesure comment l'aglo séparer les classes 1 et 0
roc_auc(log_pred, target, .pred_1, event_level = 'second') #ou comme ça
```

###4.2 Evaluer RF

```{r}
rf_pred
accuracy(rf_pred, target, .pred_class)
roc_auc(rf_pred, target, .pred_0) #mesure comment l'aglo séparer les classes 1 et 0
```

### Workflowset

```{r}
wf_set = 
workflow_set(
  preproc = list(
    basic = basic_rec,
    inter = rec_interaction,
    spline = rec_spline
  ),
  models = list(
    log = log_mod,
    rf = rf_mod,
    xg = xg_mod
  )
)
```

###validation croisée

```{r}
set.seed(42)
folds = vfold_cv(training ,v=5)

keep_pred = control_grid(save_pred = T, save_workflow = T)

set.seed(42)
res_wf_set =
wf_set %>%
  workflow_map(
    resamples = folds,
    metrics = metric_set(accuracy, roc_auc), #ex : accuracy, roc_auc, f_meas, specifity...
    control = keep_pred,
    verbose = T, #Pendant le training permet d'avoir un suivi
    grid = 20 #Par rapport au paramètres définis dans XG, il va tester X combinaisons
  )
    
rank_results(res_wf_set, rank_metric='roc_auc')
  
```
 
```{r}
best_result = 
res_wf_set %>% 
  extract_workflow_set_result('inter_xg') %>% 
  select_best(metric = 'roc_auc')
```

## 8 Best model, last fit and final prediction

```{r}
best_res_fit = 
res_wf_set %>% 
  extract_workflow('inter_xg') %>% 
  finalize_workflow(best_result) %>% 
  last_fit(split = dataset_split) #Il reconnait que c'est les données de test

best_res_fit %>%  collect_metrics()
best_res_fit %>%  collect_predictions()

```
 
 
 
 ## En résumer, il faut surtout faire la partie 1, 2 et 4
