---
title: "Projet climat LBP"
output: html_notebook
authors: "Laurent BIGAS, Mathieu DA SILVA, Killian BOULARD"
---

# Etape 1 : Déclaration des librairies
```{r warning=FALSE}
library(tidymodels)
library(tidyverse)
library(geosphere)
library(purrr)
library(lubridate)
library(FactoMineR)
library(corrplot)
library(missMDA)
library(psych)
library("writexl")
library(caret)
```

```{r setup}
knitr::opts_knit$set(root.dir = "c:/Users/User/Desktop/Cours et documents/formation R ensae/DATA")
knitr::opts_knit$set(root.dir = "C:/Users/VOYK743/Desktop/Fichiers Perso/Formation ENSAI/Datasets")
dataset = readRDS(file = "dataset.RDS")
dataset = dataset[sample(1:nrow(dataset)), ]
```

# Etape 1 : Création de l'échantillon

```{r}
rm(list = setdiff(ls(), c("dataset")))
new_data <- c("2021")
target_train_test <- c("2011", "2012","2013","2014","2015","2016","2017","2018","2019","2020")

set.seed(44)


#2.36% de Y == 1
train_test_set = dataset %>% 
  filter(annee %in% target_train_test)

new_data = dataset %>% 
  filter(annee %in% new_data)

#Les données sont déséquilibrées, donc remise en forme des proportions
#20 185 / 855 480 = 2.36%
#En appliquant la méthode X3 /3 cela donne 60 555 / 285 160 soit 21.24%
poids_presence_feu <- 10000 * 0.5
poids_non_feu <- 10000 * 0.5

echantillon_A <- train_test_set %>% filter(Y == "1") %>% sample_n(size = poids_presence_feu, replace = FALSE)
echantillon_B <- train_test_set %>% filter(Y == "0") %>% sample_n(size = poids_non_feu, replace = FALSE)
train_test_set <- bind_rows(echantillon_A, echantillon_B)
train_test_set = train_test_set[sample(1:nrow(train_test_set)), ]
rm(echantillon_A,echantillon_B)

#Au cas ou : tester sur les données fraiches
#poids_presence_feu_new_data <- 9500 * 0.2
#poids_non_feu_new_data <- 9500 * 0.8
#echantillon_A <- new_data %>% filter(Y == "1") %>% sample_n(size = #poids_presence_feu_new_data, replace = FALSE)
#echantillon_B <- new_data %>% filter(Y == "0") %>% sample_n(size = #poids_non_feu_new_data, replace = FALSE)
#new_data <- bind_rows(echantillon_A, echantillon_B)
#new_data = new_data[sample(1:nrow(new_data)), ]
#rm(echantillon_A,echantillon_B)

#Création des datasets train / test avec proportion de 80%
i <- createDataPartition(y = train_test_set$Y, times = 1, p = 0.8, list = FALSE)
training_set <- train_test_set[i,]
test_set <- train_test_set[-i,]

training_set = training_set %>% 
  select(-c(annee, code_insee, id_station))

test_set = test_set %>% 
  select(-c(annee, code_insee, id_station))

#new_dataset = dataset %>% 
#  filter(annee %in% new_data) %>% 
#  select(-c(annee, code_insee, id_station))

training_set = training_set[sample(1:nrow(training_set)), ]
test_set = test_set[sample(1:nrow(test_set)), ]
rm(train_test_set, i, poids_presence_feu,poids_non_feu, target_train_test)
```

# Etape 2 : Modélisation

```{r}
#KNN
preProcess <-  c("center", "scale", "nzv", "pca")
trControl <- trainControl(method = "repeatedcv",number = 5,repeats = 5)
trControl <- trainControl(method = "cv",number = 10)

model <- train(Y ~ ., method='knn', data = training_set, metric='Accuracy',preProcess = preProcess, trControl=trControl)

test_set$pred <- predict(model, test_set[,-which(names(test_set) == "Y")])

test_set$factor_pred <- as.factor(test_set$pred)
test_set$factor_truth <- as.factor(test_set$Y)

precision <- posPredValue(test_set$factor_truth, test_set$factor_pred)
cm <- confusionMatrix(test_set$pred, test_set$Y)


#new_data$pred <- predict(model, new_data[,-which(names(new_data) == "Y")])
#new_data$factor_pred <- as.factor(new_data$pred)
#new_data$factor_truth <- as.factor(new_data$Y)

#precision <- posPredValue(new_data$factor_truth, new_data$factor_pred)
#cm <- confusionMatrix(new_data$pred, new_data$Y)

```


```{r}
#XGB 1 : 
preProcess <-  c("center", "scale", "nzv", "pca")
xgb_trcontrol = trainControl(method = "cv", number = 5, allowParallel = TRUE, 
    verboseIter = FALSE, returnData = FALSE)

xgbGrid <- expand.grid(nrounds = c(100,200),  
                       max_depth = c(3, 5, 10, 15, 20),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       ## valeurs par défaut : 
                       eta = 0.1,
                       gamma=0,
                       min_child_weight = 1,
                       subsample = 1
                      )

set.seed(0)
xgb_model = train(Y ~ .,
                  trControl = xgb_trcontrol,
                  tuneGrid = xgbGrid, 
                  preProcess = preProcess,
                  method = "xgbTree",
                  data = training_set,
                  verbosity = 0)

test_set$pred <- predict(xgb_model, test_set[,-which(names(test_set) == "Y")])
test_set$factor_pred <- as.factor(test_set$pred)
test_set$factor_truth <- as.factor(test_set$Y)

precision <- posPredValue(test_set$factor_truth, test_set$factor_pred)
cm <- confusionMatrix(test_set$pred, test_set$Y)


#new_data$pred <- predict(xgb_model, new_data[,-which(names(new_data) == "Y")])
#new_data$factor_pred <- as.factor(new_data$pred)
#new_data$factor_truth <- as.factor(new_data$Y)

#precision <- posPredValue(new_data$factor_truth, new_data$factor_pred)
#cm <- confusionMatrix(new_data$pred, new_data$Y)

#varImp(xgb_model,scale=FALSE)
```


```{r}
#XGBOOST

#Training with xgboost - gives better scores than 'rf'
xgb_trcontrol = trainControl(method = "cv", number = 5, allowParallel = TRUE, 
    verboseIter = FALSE, returnData = FALSE)

#nrounds: nombre d’itérations de boosting à effectuer. Plus il est grand, et plus c’est lent
#max_depth: profondeur d’arbre maximale. Risque d’over-fit si trop grand, et d’under-fit si trop petit
#colsample_bytree: pourcentage des colonnes pris pour construire un arbre (rappelle-toi, un arbre est #construit avec un sous-ensemble des données: lignes et colonnes)
#eta: ou learning rate, ce paramètre contrôle la vitesse à laquelle on convergence lors de la descente du #gradient fonctionnelle (par défaut = 0.3)
#gamma: diminution minimale de la valeur de la loss (fonction objectif) pour prendre la décision de #"partitionner une feuille

xgbGrid <- expand.grid(nrounds = 100,  
                       max_depth = c(3, 5, 10, 15, 20),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       ## valeurs par défaut : 
                       eta = 0.1,
                       gamma=0,
                       min_child_weight = 1,
                       subsample = 1
                      )

xgb_model = train(Y ~.,
                  data = training_set,
                  trControl = xgb_trcontrol,
                  tuneGrid = xgbGrid, 
                  method = "xgbTree",
                  verbosity = 0)


test_set$pred <- predict(xgb_model, test_set[,-which(names(test_set) == "Y")])
test_set$factor_pred <- as.factor(test_set$pred)
test_set$factor_truth <- as.factor(test_set$Y)

precision <- posPredValue(test_set$factor_truth, test_set$factor_pred)
cm <- confusionMatrix(test_set$pred, test_set$Y)

cm
```

```{r}
#RandomForest
preProcess <-  c("center", "scale", "nzv", "pca")

param_grid <- expand.grid(
  ntree = seq(50, 200, 50),
  mtry = seq(2, 6, 1),
  max_depth = seq(5, 15, 2)
)
model <- train(Y ~ ., data = training_set, method = "rf",
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = param_grid)
```

